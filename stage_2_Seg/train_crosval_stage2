#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Oct 19 13:23:04 2023

@author: juwita
"""

import pickle
import os
import numpy as np
import cv2
from tqdm import tqdm
import time
from os import path
import albumentations as A
from albumentations import HorizontalFlip, VerticalFlip, Rotate, ShiftScaleRotate, CenterCrop, GaussianBlur, HueSaturationValue, Transpose
from sklearn.model_selection import KFold, cross_val_score
from metrics import dice_loss, dice_coef, Jaccard, iou
from glob import glob
from metric_auc_spec import weighted_bce, specificity, specificity_score, calculate_auc
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
import tensorflow as tf 
from sklearn.utils import shuffle
from tensorflow.keras.metrics import Recall, Precision
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard
from keras.models import load_model
import tensorflow.keras.backend as K 
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import make_scorer
from tensorflow.keras.utils import CustomObjectScope
import re
import csv


def create_dir(path):
    """ Create a directory. """
    if not os.path.exists(path):
        os.makedirs(path)


def shuffling(x, y):
    x, y = shuffle(x, y, random_state=42)
    return x, y

def load_data(path):
    #x = sorted(glob(os.path.join(path, "image", "*.png")))
    x = sorted(glob(os.path.join(path, "image/img", "*.png")))
    y = sorted(glob(os.path.join(path, "label/img", "*.png")))
    return x, y



def read_image(paths):
    images = []
    for path in paths:
    #path = path.decode()
        x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
        x = cv2.resize(x,(H,W))
        x = x/255.0
        x = x.astype(np.float32)
        x = np.expand_dims(x, axis=-1)
        x = np.repeat(x, 3, axis=2)
        images.append(x)
    return images

def read_mask(paths):
    masks = []
    for path in paths:
    #path = path.decode()
        x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
        x = cv2.resize(x,(H,W))
        x = x/255.0
        x = x > 0.5
        x = x.astype(np.float32)
        x = np.expand_dims(x, axis=-1)
        x = np.repeat(x, 3, axis=2)
        masks.append(x)
    return masks

def tf_parse(x, y):
    def _parse(x, y):
        
        return x, y

    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])
    x.set_shape([H, W, 3])  #ini nanti diubah kalo mau 3 channel
    y.set_shape([H, W, 3])
    return x, y



def tf_parse_augment(x, y):
    def _parse_augment(x, y):
       
        x = np.array(x)
        y = np.array(y)
        
       

        
        augment = A.Compose([
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.RandomRotate90(p=0.5) #,A.GaussianBlur(p=1)
        ], p=1)

        augmented = augment(image=x, mask=y)
        x_augmented = augmented["image"]
        y_augmented = augmented["mask"]

        return x_augmented, y_augmented

    x, y = tf.numpy_function(_parse_augment, [x, y], [tf.float32, tf.float32])
    x.set_shape([H, W, 3])
    y.set_shape([H, W, 3])
    return x, y




def tf_dataset(images, mask, augment = True, batch=8):
    dataset = tf.data.Dataset.from_tensor_slices((images, mask))
    if augment:
        dataset = dataset.map(tf_parse_augment)
        print("dengan augment",len(dataset))
    else:
        dataset = dataset.map(tf_parse)
        print("tanpa augment", len(dataset))
    dataset = dataset.batch(batch)
    dataset = dataset.prefetch(10)
    return dataset


if __name__ == "__main__":
    """ Load the dataset """
    H = 128
    W = 128

    base_path = "your path"
    image_path = os.path.join(base_path, "image", "img")
    mask_path = os.path.join(base_path, "label", "img")

    num_runs= 1
    for run in range(num_runs):
        run_folder = f'run_{run + 1}'  
        os.makedirs(run_folder, exist_ok=True)  

   
        os.chdir(run_folder)

        print(f"Run {run + 1}:")

        """ Seeding """
        np.random.seed(42)
        tf.random.set_seed(42)
    
        """ Directory for storing files """
        create_dir("files1")
    
        """ Hyperparameters """
        batch_size = 8
        model_name = "give name"
        model_path = os.path.join("files1", "model" + str(model_name) + ".h5")
        time_path = os.path.join("files1","times.csv" )
        
        
        with open(time_path, 'w', newline='') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['Epoch', 'Waktu (detik)']) 
         
    
        k =4 
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
    
        image_names = os.listdir(image_path)
        image_names = sorted(set([name.split("-")[0] for name in image_names]))
    
        for fold, (train_idx, val_idx) in enumerate(kf.split(image_names)):
            print(f"Fold {fold+1}")
            
            train_names = [image_names[i] for i in train_idx]
            test_names = [image_names[i] for i in test_idx]
    
            train_images, train_masks = [], []
            test_images, test_masks = [], []
    
            for name in train_names:
                print(f'Training set patient: {name}')
               
                image_files = sorted(glob(os.path.join(image_path, name + "-slice*_*.png")))
                mask_files = sorted(glob(os.path.join(mask_path, name + "-slice*_*.png")))
                
                images = read_image(image_files)
                           
                masks= read_mask(mask_files)
                train_images.extend(images)
                train_masks.extend(masks)
    
            for name in test_names:
                print(f'Validation set patient: {name}')
                
                image_files = sorted([f for f in glob(os.path.join(image_path, name + "-slice*_*.png")) if  (f.endswith("_x.png") )])
                mask_files = sorted([f for f in glob(os.path.join(mask_path, name + "-slice*_*.png")) if  (f.endswith("_x.png") )])  
                images_test = read_image(image_files)
                mask_test = read_mask(mask_files)
                test_images.extend(images_val)
                test_masks.extend(mask_val)
            
    
            print(f"Train set: {len(train_images)} images, {len(train_masks)} masks")
            print(f"test set: {len(test_images)} images, {len(test_masks)} masks")
    
            print(f"Start training for fold {fold}")
                                       
            
            train_dataset = tf_dataset(train_images, train_masks, augment=True, batch=batch_size)
            test_dataset = tf_dataset(val_images, val_masks, augment=False, batch=batch_size)
            
    
            K.clear_session()
            with tf.device('/GPU:0'):
                H = 128
                W = 128
                C = 3
                input_shape = (H,W,C)
                initial_learning_rate = 0.001
                
                fine_tune_learning_rate = 0.0001 
                initial_epochs = 10
                fine_tune_epochs =200
                
                model = build_model(H,W,C, encoder_trainable = False)
                
                metrics = [calculate_auc, specificity, iou, dice_coef, dice_loss, Recall(), Precision()]
                                        
    
                scorer = {"dice_coef": make_scorer(dice_coef), 'iou': make_scorer(iou),
                                     "dice_loss":make_scorer(dice_loss)}
                                                                          
    
                     
    
                model.compile(loss=dice_loss, optimizer=Adam(learning_rate=initial_learning_rate), metrics=metrics)
    
                callbacks = [
                    ModelCheckpoint(model_path, monitor="val_loss", verbose=1, save_best_only=True, save_weights_only= True),
                    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-8, verbose=1),
                    CSVLogger(csv_path),
                    TensorBoard(log_dir="log_" + tensor_path + "/"),
                    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
                ]
                
                start_time = time.time()
                history = model.fit(
                   
                            train_dataset,
                            epochs=initial_epochs,
                            validation_data=test_dataset,
                            callbacks=callbacks,
                            shuffle=True)
    
                end_time = time.time()
                training_duration=end_time-start_time
                print(f"Training (Stage 1) took {training_duration:.2f} seconds")
    
 #stage 2 
    
    
    
    
                model = build_model(H,W,C, encoder_trainable = True)
                model.compile(loss=dice_loss, optimizer=Adam(learning_rate=fine_tune_learning_rate), metrics=metrics)
                
                
                start_time = time.time()
                history = model.fit(
               
                        train_dataset,
                        epochs=fine_tune_epochs,
                        
                        callbacks=callbacks,
                        shuffle=True)
                
                loss_and_metrics = model.evaluate(test_dataset, verbose=0)

                loss_eval = loss_and_metrics[0]
                calculate_auc_eval = loss_and_metrics[1]
                specificity_eval = loss_and_metrics[2]
                iou_eval = loss_and_metrics[3]
                dice_coef_eval = loss_and_metrics[4]
                dice_loss_eval = loss_and_metrics[5]
                recall_eval = loss_and_metrics[6]
                precision_eval = loss_and_metrics[7]
        
       
                metrics_to_save = [
                    loss_eval, calculate_auc_eval, specificity_eval, iou_eval, dice_coef_eval, dice_loss_eval, recall_eval, precision_eval
                ]
        

                file_exists = os.path.isfile(filename)
                headers = ["Loss", "AUC", "Specificity", "IoU", "Dice Coefficient", "Dice Loss", "Recall", "Precision"]
                    with open(filename, 'a', newline='') as csvfile:
                        writer = csv.writer(csvfile)
    
    # if file none:
            if not file_exists:
                writer.writerow(headers)
    
    # write metric:
            writer.writerow(metrics_to_save)
        
        
            
            end_time = time.time()
            training_duration = end_time - start_time
            print(f"Training (Stage 2) took {training_duration:.2f} seconds")
            
            with open(time_path, 'a', newline='') as csvfile:
                csv_writer = csv.writer(csvfile)
                csv_writer.writerow(['Total Training (Stage 1)', training_duration])
                csv_writer.writerow(['Total Training (Stage 2)', training_duration])
        
    
       
    
    
        os.chdir('..')  
    print("Training done.")


