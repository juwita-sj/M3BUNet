#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Mar  1 03:50:54 2023

@author: juwita
"""


import tensorflow as tf 
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, UpSampling2D
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, Multiply, LeakyReLU
import numpy as np 
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import Recall, Precision 
from tensorflow.keras import backend as K 
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dropout


print("TF Version: ", tf.__version__)

#---------------------------------Seeding-----------------------------#
np.random.seed(42)
tf.random.set_seed(42)






def spatial_attention(x):
    #average pooling 
    x1 = tf.reduce_mean(x, axis = -1) #ini dari yt, untuk ambil dua dimensi saja? 
    x1 = tf.expand_dims (x1, axis = -1) # dari yt(idiot..) 
    
    #MaxPooling
    x2 = tf.reduce_max (x, axis = -1)
    x2 = tf.expand_dims(x2, axis = -1 )
   
    #concatenate 
    comb = Concatenate()([x1,x2])
    
    #convLayer 
    comb = Conv2D(1, kernel_size = 3, padding="same", activation = "sigmoid")(comb)
    comb = Multiply()([x,comb])
    
    return comb
    


def combine_attention(x):
    x1 = spatial_attention(x)
    f_a = x1
    f_b = Conv2D(1, (1, 1), padding='same', use_bias=False)(x)
    f_act = tf.keras.layers.Activation('sigmoid')(f_b)
  
    combined = Concatenate()([f_act, f_a])
             
    combined = Conv2D(128, kernel_size=3, padding='same')(combined) 
    combined = BatchNormalization()(combined)
    combined = Activation('relu')(combined)
    
    combined = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid')(combined) 
    combined = Multiply()([x, combined]) 
    
   
    combined = Conv2D(128, kernel_size=1, padding='same')(combined) 
    combined = BatchNormalization()(combined)
    combined = Activation('relu')(combined)
    
    return combined




#---------------------------------Model -----------------------------#

def build_model(H,W,C): 
    inputs = Input(shape=(H, W, C), name = "input_image")
    
    
    encoder = MobileNetV2(include_top=False, weights="imagenet",
        input_tensor=inputs)
    
        
    #encoder.trainable = encoder_trainable
    # for layer in encoder.layers:
    #     layer.trainable = False
    
    skip_connection_names = ["input_image", "block_1_expand_relu", "block_3_expand_relu", "block_6_expand_relu"]
   
    encoder_output = encoder.get_layer ("block_13_expand_relu").output  #(16x16) 
   
    
    #f = [16,32,48,64]  ##ini kenapa 48? bisa ga kita ubah jadi 
    f = [16,32,64,128]
    x =encoder_output
    #ini yang aku tambahkan tanggal 06/03
    x = Conv2D(1, kernel_size=3, padding='same')(x) # ubah juga dari 48 ke 48 
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
   
    
# Selanjutnya, Anda dapat menambahkan lapisan konvolusi atau lapisan lainnya sesuai kebutuhan

    
      
    
   
   
    # x = Conv2D(1, kernel_size=5, padding='same')(x) # ubah juga dari 48 ke 48 
    # x = BatchNormalization()(x)
    # x = Activation('relu')(x)
   
    
    # x = Conv2D(1, kernel_size=3, padding='same')(x) # ubah juga dari 48 ke 48 
    # x = BatchNormalization()(x)
    # x = Activation('relu')(x)
   
    
      
    
    # x = Conv2D(1, (3,3),padding = 'same' )(x) 
    # x = BatchNormalization()(x)
    # x = Activation("relu")(x)
    
   # x = Dropout(0.1)(x)
    x = combine_attention(x)
    x = Dropout(0.2)(x)
    
    #encoder.summary()
    
    for i in range(1,len(f)+1, 1):
        x_skip = encoder.get_layer(skip_connection_names[-i]).output 
        x = UpSampling2D((2,2))(x)
        x = Concatenate()([x,x_skip])
        
        x = combine_attention(x)
        #ini yang aku tambahkan tanggal 06/03
        x = Dropout(0.2)(x)
        
        x = Conv2D(f[-i],(3,3),padding = "same")(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
        #x = Dropout(0.2)(x)
        
        
        x = Conv2D(f[-i],(3,3),padding = "same")(x)
        x = BatchNormalization()(x)
        x = Activation("relu")(x)
        #x = Dropout(0.5)(x)
        
    #output
    x = Conv2D(3, (1,1), padding="same")(x)
    x = Activation("sigmoid")(x)
    
    model = Model(inputs,x)
    return model 


   
    




if __name__ == "__main__":
    H = 64
    W =64
    C= 3
    input_shape = 64
    model = build_model(H,W,C)
    model.summary()
    #execute 
    # inputs = Input(shape = (128,128,1))
    # inputss = (128,128,1)
    # y = Quick_Spatial(inputs)
    # print ((y.shape))

   
